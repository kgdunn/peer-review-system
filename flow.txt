
* Statistics: getting review
* Statistics: getting grades

How to resolve POST errors: ""request data read error""
Debug logs: show which PRProcess is being view/assessed etc
----

* Seems like it sets other's work as is_valid=False
* Thumbnailing is wrong


* Raise error messages: incorrect filetypes, cannot create PDF, etc
* Create a tmbnail from PDF
* Submission filename is forced into a template
* Allow multiple uploads, but bundle them on the back-end into 1 PDF file
* One dropdown: 4, 6, 8, 10
* Feedback box
* Student receives email when feedback is available
* Just a warning: if you upload again; your feedback is overwritten
* During the upload/submission phase: admin sees which groups have/haven't uploaded

---
brew install imagemagick@6
ln -s /usr/local/Cellar/imagemagick\@6/6.9.8-3/lib/libMagickWand-6.Q16.dylib /usr/local/lib/libMagickWand.dylib

----
* See their progress in the peer/self review process: xx % completed
* Push the grades back to Brightspace
* Standard deviation of std. deviation to Conny
* Show in the admin interface the avg time take on the rubric

https://community.brightspace.com/devcop/blog/setting_up_an_lti_11_integration_in_brightspace

* Return grade to edX?


* What was the effect of self-review? Did reports improve?
* Send ZIP file of all practicum reports: self-review and peer-review

* Model to store grade reports in the database; with a flag to capture updates

* Students don't get a grade if they have not submitted peer review'

* Show admin's grades as soon as they graded

* How the correlation of the instructors matches up with the students.
* D3JS histogram of the grades for the students
* Add a "header" section inbetween items. To divide sections up.
* Add XHR for checkbox items
* Get on a per-item basis, the std dev for Conny to help to improve rubric.
* Calculate the colour coded correlation (coloured by instructor name)

Show progress of work done on a page (NN% completed)
Save the r_actual at every XHR, so the .modified field is updated.
Show r_actual in group.ID order, or submission order
Check that peer feedback results are the correct ones



* Log the XHR events with Statistics still

* Completed submissions via an intermediate database table

# And also mark the submission as having one extra submission:
#r_actual.submission.number_reviews_completed += 1
#r_actual.submission.status = 'G' # in progress
#r_actual.submission.save()

# Reviews to still complete by this learner:
#n_graded_already = RubricActual.objects.filter(graded_by=learner,
#                                               status='C').count()
#phase = r_actual.rubric_template.phase
#n_to_do = max(0, get_n_reviews(learner, phase) - n_graded_already)

Copy rubric over (in the shell)
----------------
from review.models import PR_process, RubricTemplate, RItemTemplate
src_template = RubricTemplate.objects.get(title='Repair practicum self-review')
dst_template = RubricTemplate.objects.get(title='Repair practicum peer-review')
src_items = RItemTemplate.objects.filter(r_template=src_template)
for item in src_items:
    # First get the associated options
    options = item.roptiontemplate_set.all()
    
    # Then copy the parent template to the new one
    item.pk = None
    item.r_template = dst_template
    item.save()
    
    # Then re-parent the options to the newly created/saved item
    for opt in options:
        opt.pk = None
        opt.rubric_item = item
        opt.save()
   
    
    
To create a new rubric
-----------
0. Create a course, with the LTI connection
1. Create the new LTI link item in Brightspace. It will reveal the "resource_link_id"
2. Use that code, to create a new PR_Process: give it a name, attach it to an
   existing course, select group formation process.

3. Create a a new "RubricTemplate" instance, associated with this PR_Process.
   Fill in the maximum score; 
4. Create one or more phases (submission phase, etc). Remember to create these
   directly from the source model. i.e. create a SubmissionPhase, and not a
   PRPhase of the submission type
   
5. When you create the self-review (or peer-review phase), you need to link it to
   the appropriate RubricTemplate, created above. Create the phase, but it will
   crash in the browser, until you go the create RubricTemplate, and mention
   the phase to which it is attached. Now it will succeed in the browser.
   
6. For either the self- or peer-review you need item(s), and each item has 1
   or more associated options. Enter these, as RItemTemplate's and 
   ROptionTemplate's based on the instructor's rubric.
  


TO DO
======
* Group email: make an HTML template. Allow the template to be in the PR_Process?
* Does it matter that there are duplicate "Enrolled" instances?
* Don't allow non-PDF for upload
* Return error messages for non-PDF; too large files

Groups
--------
Open time
Closing time
Allow more than N enrollments
Groups of different sizes
Can be prefilled
Allow self-removal and re-enrollment
Longer term: waitlist
Export to CSV
IMport names from CSV
Show description of groups
Log for administrator

Upload
=-------
Add upload bar: https://github.com/ouhouhsami/django-progressbarupload
Size limit
Strip out the PDF information
Check for PDF
>>> import magic
>>> magic.from_file("testdata/test.pdf")


To do next
------
Prominently colour "Submit" button. Orange when not submitted; Green when submitted.


During the run:
----
Watch the reviews coming in at: https://peer.connectmv.com/admin/review/roptionactual/
Track linode stats
Run htop
Dump the data to text file:
  ./manage.py dumpdata review stats groups --format=json --indent=2 > backup-`date '+%Y-%m-%d-%H-%M-%S'`.json


Review process
==============
Use the LTI checker: http://ltiapps.net/test/tc.php
https://github.com/Brightspace/sample-LTI-WHMIS-quiz
Send grades back:
https://community.brightspace.com/devcop/blog/so_you_want_to_extend_your_lms__part_1_lti_primer

To send back grades once a student has completed an assessment in a learning tool, the tool sends an HTTP POST back to the LMS via the URL specified in the [lis_outcome_service_url] field, the payload being a block of XML that contains the grade details.

<?xml version = "1.0" encoding = "UTF-8"?>
<imsx_POXEnvelopeRequest xmlns = "http://www.imsglobal.org/services/ltiv1p1/xsd/imsoms_v1p0">
  <imsx_POXHeader>
    <imsx_POXRequestHeaderInfo>
      <imsx_version>V1.0</imsx_version>
      <imsx_messageIdentifier>51b8cdfc2a7bb</imsx_messageIdentifier>
    </imsx_POXRequestHeaderInfo>
  </imsx_POXHeader>
  <imsx_POXBody>
    <replaceResultRequest>
      <resultRecord>
        <sourcedGUID>
          <sourcedId>8d053b33-6f1f-410c-888b-1944226e44d3</sourcedId>
        </sourcedGUID>
        <result>
          <resultScore>
            <language>en-us</language>
            <textString>0.33</textString>
          </resultScore>
        </result>
      </resultRecord>
    </replaceResultRequest>
  </imsx_POXBody>
</imsx_POXEnvelopeRequest>


Look at extending it with a remote plugin:
http://docs.valence.desire2learn.com/ui-ext/rplugins.html