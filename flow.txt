TO DO
======

Clean slate for allocations and completions
Check calculations are correct and
Process results of (numeric + text)evaluations to display back to students


Is there a miscount here? ``for k in range(n_reviews - query.count()):``

Relocate to actual course
Test the actual process: from beginning to end.


Show the score they awarded in the peer review.


Prominently colour "Submit" button. Orange when not submitted; Green when submitted.
Allow resubmissions: ideally reload the prior submitted values
Polling the server on a minute by minute basis for status update on the review
After cut-off time show calculation of their grades

Shows in the HTML if it has been successfully submitted.
Replicate the infrastructore on Webfaction
Show review as being completed in the browser

Track review text and selections immediately in the database
Allow multiple attempts for the review, until the cut-off point is reached.
Strip out and PDF information.


Before the run:
----
Reduce memory for yint
Reduce memory for rsmopt

During the run:
----
Watch the reviews coming in at: https://peer.connectmv.com/admin/review/roptionactual/
Track linode stats
Run htop
Dump the data to text file:
  ./manage.py dumpdata review  --format=json --indent=2 > backup-`date '+%Y-%m-%d-%H-%M-%S'`.json



Review process
==============
in a responsive way that the rubric goes below the document
rubric answers are saved as entered in realtime
resume /save as draft/submit
Cannot edit/resubmit after submission date
If the page is reloaded, it shows the options from last time

* Handle self-review still
"""