How well is the goal defined?

10 Extraordinary
9 - Excellent
8- Good
7 - Sufficient
5 â€“ Insufficient
4- not delivered
 

On edx we describe it as follows:

You will receive feedback based on the richness of your scope, but also in relation to a manageable focus. The staff will also check if you took an experience-centered perspective for your research goal, instead of a focus on a product. 



----
* Get the tables sortable in the view
* Get 2 late peer review links out to the students.
* Statistics: getting review
* Statistics: getting grades
* PDF magick checker
* Why do the RActuals get assigned for phase 2?

---
ASsert the lenght of RItemActuals is the same (around line 377)
items = completed_review.ritemactual_set.filter(submitted=True)\
                                                            .order_by('id')
        overall_max_score = 0.0 # just compute this on the last completed_review
        for idxi, item in enumerate(items):
        
---

Check: is it true that if you add a new Item, that existing RActuals are not 
       updated?

How to resolve POST errors: ""request data read error""
----
Use this for tables: http://olifolkerd.github.io/tabulator/examples/
----

TO DO
======
* Group email: make an HTML template. Allow the template to be in the PR_Process?
* Don't allow non-PDF for upload
* Return error messages for non-PDF; too large files


To do next
------
Prominently colour "Submit" button. Orange when not submitted; Green when submitted.



Upload
=-------
Add upload bar: https://github.com/ouhouhsami/django-progressbarupload
http://stackoverflow.com/questions/166221/how-can-i-upload-files-asynchronously
Size limit
Strip out the PDF information
Check for PDF
>>> import magic
>>> magic.from_file("testdata/test.pdf")


* Raise error messages: incorrect filetypes, cannot create PDF, etc
* Create a tmbnail from PDF
* Submission filename is forced into a template
* Allow multiple uploads, but bundle them on the back-end into 1 PDF file
* One dropdown: 4, 6, 8, 10
* Feedback box
* Student receives email when feedback is available
* Just a warning: if you upload again; your feedback is overwritten
* During the upload/submission phase: admin sees which groups have/haven't uploaded

---
brew install imagemagick@6
ln -s /usr/local/Cellar/imagemagick\@6/6.9.8-3/lib/libMagickWand-6.Q16.dylib /usr/local/lib/libMagickWand.dylib

----
* Statistics: time from first opening, to first click of a piece of feedback/score
* Stats/check: number of words per box (normalized) plot again instructor score/peer score
* Number of words vs time taken

* See their progress in the peer/self review process: xx % completed
* Push the grades back to Brightspace
* Standard deviation of std. deviation to Conny
* Show in the admin interface the avg time take on the rubric

https://community.brightspace.com/devcop/blog/setting_up_an_lti_11_integration_in_brightspace
----
Research question: did self-review help? Grades increased? Quality of reports up?
----
* Return grade to edX?


* What was the effect of self-review? Did reports improve?
* Send ZIP file of all practicum reports: self-review and peer-review

* Model to store grade reports in the database; with a flag to capture updates

* Students don't get a grade if they have not submitted peer review'

* Show admin's grades as soon as they graded

* How the correlation of the instructors matches up with the students.
* D3JS histogram of the grades for the students
* Add a "header" section inbetween items. To divide sections up.
* Add XHR for checkbox items
* Get on a per-item basis, the std dev for Conny to help to improve rubric.
* Calculate the colour coded correlation (coloured by instructor name)

Show progress of work done on a page (NN% completed)
Save the r_actual at every XHR, so the .modified field is updated.
Show r_actual in group.ID order, or submission order
Check that peer feedback results are the correct ones


Copy rubric over (in the shell)
----------------
./manage.py shell
from review.models import PR_process, RubricTemplate, RItemTemplate
src_template = RubricTemplate.objects.get(title='Recyclability practicum self-review')
dst_template = RubricTemplate.objects.get(title='Recyclability practicum peer-review')
src_items = RItemTemplate.objects.filter(r_template=src_template)
for item in src_items:
    # First get the associated options
    options = item.roptiontemplate_set.all()
    
    # Then copy the parent template to the new one
    item.pk = None
    item.r_template = dst_template
    item.save()
    
    # Then re-parent the options to the newly created/saved item
    for opt in options:
        opt.pk = None
        opt.rubric_item = item
        opt.save()
   
To create a review for a student past the deadline
---------
./manage.py shell
from review.models import PRPhase, Person, RubricTemplate
from review.views import get_next_submission_to_evaluate

self = PRPhase.objects.get(id=___) <- the phase for  #20
learner = Person.objects.get(id=___) <- the student
r_template = RubricTemplate.objects.get(id=5)
next_subs = get_next_submission_to_evaluate(self, learner, return_all=True)

# Hit database to get the next submission to grade:
next_sub = next_subs[0]
r_actual, new = get_create_actual_rubric(learner=learner,
                                         template=r_template,
                                         submission=next_sub)

if new:
    next_sub.number_reviews_assigned += 1
    next_sub.save()
    r_actuals.append(r_actual)


    
To create a new rubric
-----------
0. Create a course, with the LTI connection
1. Create the new LTI link item in Brightspace. It will reveal the "resource_link_id"
2. Use that code, to create a new PR_Process: give it a name, attach it to an
   existing course, select group formation process.

3. Create a a new "RubricTemplate" instance, associated with this PR_Process.
   Fill in the maximum score; 
4. Create one or more phases (submission phase, etc). Remember to create these
   directly from the source model. i.e. create a SubmissionPhase, and not a
   PRPhase of the submission type
   
5. When you create the self-review (or peer-review phase), you need to link it to
   the appropriate RubricTemplate, created above. Create the phase, but it will
   crash in the browser, until you go the create RubricTemplate, and mention
   the phase to which it is attached. Now it will succeed in the browser.
   
6. For either the self- or peer-review you need item(s), and each item has 1
   or more associated options. Enter these, as RItemTemplate's and 
   ROptionTemplate's based on the instructor's rubric.
  


Review process
==============
Use the LTI checker: http://ltiapps.net/test/tc.php
https://github.com/Brightspace/sample-LTI-WHMIS-quiz
Send grades back:
https://community.brightspace.com/devcop/blog/so_you_want_to_extend_your_lms__part_1_lti_primer

To send back grades once a student has completed an assessment in a learning tool, the tool sends an HTTP POST back to the LMS via the URL specified in the [lis_outcome_service_url] field, the payload being a block of XML that contains the grade details.

<?xml version = "1.0" encoding = "UTF-8"?>
<imsx_POXEnvelopeRequest xmlns = "http://www.imsglobal.org/services/ltiv1p1/xsd/imsoms_v1p0">
  <imsx_POXHeader>
    <imsx_POXRequestHeaderInfo>
      <imsx_version>V1.0</imsx_version>
      <imsx_messageIdentifier>51b8cdfc2a7bb</imsx_messageIdentifier>
    </imsx_POXRequestHeaderInfo>
  </imsx_POXHeader>
  <imsx_POXBody>
    <replaceResultRequest>
      <resultRecord>
        <sourcedGUID>
          <sourcedId>8d053b33-6f1f-410c-888b-1944226e44d3</sourcedId>
        </sourcedGUID>
        <result>
          <resultScore>
            <language>en-us</language>
            <textString>0.33</textString>
          </resultScore>
        </result>
      </resultRecord>
    </replaceResultRequest>
  </imsx_POXBody>
</imsx_POXEnvelopeRequest>


Look at extending it with a remote plugin:
http://docs.valence.desire2learn.com/ui-ext/rplugins.html